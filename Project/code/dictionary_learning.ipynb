{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import time\n",
    "import shelve\n",
    "import pickle\n",
    "import os.path as op\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn import linear_model, datasets\n",
    "\n",
    "from nt_toolbox.signal import load_image, imageplot, snr\n",
    "from nt_toolbox.general import clamp\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: quote the matlab numerical tour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def damage_image(image):\n",
    "    rho = .7 # percentage of removed pixels\n",
    "    Omega = np.zeros([img_size, img_size])\n",
    "    sel = np.random.permutation(img_size**2)\n",
    "    np.ravel(Omega)[sel[np.arange(int(rho*img_size**2))]] = 1\n",
    "\n",
    "    Phi = lambda f, Omega: f*(1-Omega)\n",
    "\n",
    "    damaged_image = Phi(image, Omega)\n",
    "    return damaged_image\n",
    "\n",
    "def random_dictionary(image, width, n_atoms):\n",
    "    '''\n",
    "    Takes an image as input and returns a dictionary\n",
    "    of shape (width*width, n_atoms)\n",
    "    '''\n",
    "    assert image.shape[0] == image.shape[1]\n",
    "    n0 = image.shape[0]\n",
    "    \n",
    "    # Random sampling of coordinates of the top left corner or the patches\n",
    "    x = (np.random.random((1,1,n_atoms))*(n0-width)).astype(int)\n",
    "    y = (np.random.random((1,1,n_atoms))*(n0-width)).astype(int)\n",
    "    \n",
    "    # Extract patches\n",
    "    [dY,dX] = np.meshgrid(range(width), range(width))\n",
    "    dX = np.tile(dX, (n_atoms,1,1)).transpose((1,2,0))\n",
    "    dY = np.tile(dY, (n_atoms,1,1)).transpose((1,2,0))\n",
    "    Xp = np.tile(x, (width,width,1)) + dX\n",
    "    Yp = dY + np.tile(y, (width,width,1))\n",
    "    D = image.flatten()[Yp+Xp*n0]\n",
    "    D = D.reshape((width*width,n_atoms)) # Reshape from (w,w,q) to (w*w,q)\n",
    "    return D\n",
    "\n",
    "def center(D):\n",
    "    '''\n",
    "    Takes a Dictionary of shape (signal_size, n_atoms) and\n",
    "    substract the signal-wise mean.\n",
    "    '''\n",
    "    assert len(D.shape) == 2\n",
    "    D -= D.mean(axis=0)\n",
    "    return D\n",
    "\n",
    "def scale(D):\n",
    "    ''' Scale the dictionary atoms to unit norm '''\n",
    "    assert len(D.shape) == 2\n",
    "    norm = np.tile(np.linalg.norm(D, axis=0), (D.shape[0],1))\n",
    "    D = np.divide(D, norm)\n",
    "    return D\n",
    "\n",
    "def high_energy_random_dictionary(image, width, n_atoms):\n",
    "    '''\n",
    "    Initialize a random dictionary with high energy centered and \n",
    "    normalized atoms  of size (width*width, n_atoms)\n",
    "    '''\n",
    "    m = 20*n_atoms\n",
    "    q = 3*m\n",
    "    D = random_dictionary(image, width, q)\n",
    "    D = center(D)\n",
    "    # Keep patches with highest energy\n",
    "    energies = np.sum(D**2, axis=0)\n",
    "    Indexes = np.argsort(energies)[::-1]\n",
    "    D = D[:,Indexes[:m]]\n",
    "    # Select a random subset of these patches\n",
    "    sel = np.random.permutation(range(m))[:n_atoms]\n",
    "    D = D[:,sel]\n",
    "    D = scale(D)\n",
    "    return D\n",
    "\n",
    "def plot_dictionary(D):\n",
    "    ''' Plot a dictionary of shape (width*width, n_atoms) '''\n",
    "    # Check that D.shape == (width*width, n_atoms)\n",
    "    assert len(D.shape) == 2\n",
    "    assert int(np.sqrt(D.shape[0]))**2 == D.shape[0]\n",
    "    (signal_size, n_atoms) = D.shape\n",
    "    width = int(np.sqrt(D.shape[0]))\n",
    "    D = D.reshape((width,width,n_atoms))\n",
    "    n = int(np.ceil(np.sqrt(n_atoms))) # Size of the plot square in number of atoms\n",
    "\n",
    "    # Pad the atoms\n",
    "    pad_size = 1\n",
    "    missing_atoms = n ** 2 - n_atoms\n",
    "\n",
    "    padding = (((pad_size, pad_size), (pad_size, pad_size),\n",
    "                (0, missing_atoms)))\n",
    "    D = np.pad(D, padding, mode='constant', constant_values=1)\n",
    "    padded_width = width + 2*pad_size\n",
    "    D = D.reshape(padded_width,padded_width,n,n)\n",
    "    D = D.transpose(2,0,3,1) # Needed for the reshape\n",
    "    big_image_size = n*padded_width\n",
    "    D = D.reshape(big_image_size, big_image_size)\n",
    "    imageplot(D)\n",
    "\n",
    "def ProjX(X,k):\n",
    "    ''' Sparsity projection, keeps the k largest coefficients '''\n",
    "    X = X * (abs(X) >= np.sort(abs(X), axis=0)[-k,:])\n",
    "    return X\n",
    "\n",
    "\n",
    "def ProjC(D):\n",
    "    ''' Dictionary projection, scales the atoms '''\n",
    "    D = scale(D)\n",
    "    return D\n",
    "\n",
    "def sparse_code_pgd(Y, D, X, sparsity=4, n_iter=100):\n",
    "    '''\n",
    "    Sparse code data Y using dictionary D using a forward backward iterative scheme.\n",
    "    This is a non-smooth and non-convex minimization, that can be shown to be NP-hard.\n",
    "    A heuristic to solve this method is to compute a stationary point of the energy\n",
    "    using the Foward-Backward iterative scheme (projected gradient descent).\n",
    "    '''\n",
    "    gamma = 1/np.linalg.norm(np.dot(D,D.T)) # TODO: Improve gamma ? (compare with nt)\n",
    "    for i in range(n_iter):\n",
    "        R = np.dot(D, X) - Y\n",
    "        X = ProjX(X - gamma * np.dot(D.T, R), sparsity)\n",
    "    return X\n",
    "\n",
    "def sparse_code_lasso(Y, D, model):\n",
    "    ''' Sparse code data Y using dictionary D using lasso linear regression '''\n",
    "    X = lasso.fit(D, Y).coef_.T\n",
    "    return X\n",
    "\n",
    "def dictionary_update_ksvd(Y, D, X):\n",
    "    (signal_size, n_atoms) = D.shape\n",
    "    (_, n_samples) = Y.shape\n",
    "\n",
    "    for k in tqdm(n_atoms):\n",
    "        dk = D[:,k].reshape((signal_size, 1))\n",
    "        xk = X[k,:].reshape((1, n_samples))  # Careful, this is the kth ROW and not kth column\n",
    "\n",
    "        # Error part that does not depend on dk\n",
    "        Ek = Y - (np.dot(D,X) - (np.dot(dk,xk)))\n",
    "\n",
    "        # Samples that use atom dk\n",
    "        omega_k = np.where(xk != 0)[1]\n",
    "        xk_R = xk[:,omega_k]\n",
    "        Ek_R = Ek[:,omega_k]\n",
    "        U, s, V = np.linalg.svd(Ek_R)\n",
    "\n",
    "        # Update dk column and xk row\n",
    "        D[:,k] = U[:,0]\n",
    "        X[k,omega_k] = s[0] * V[:,0]\n",
    "\n",
    "    return D, X\n",
    "\n",
    "def dictionary_update_pgd(Y, D, X, n_iter=50):\n",
    "    tau = 1/np.linalg.norm(np.dot(X, X.T)) # TODO: Improve tau ? (compare with nt)\n",
    "    for i in range(n_iter):\n",
    "        R = np.dot(D, X) - Y\n",
    "        D = ProjC(D - tau * np.dot(R, X.T))\n",
    "    return D\n",
    "\n",
    "def dictionary_update_omf(D, A, B):\n",
    "    '''\n",
    "    Algorithm 2 from \"Online Learning for Matrix Factorization and Sparse Coding\n",
    "    Update the dictionary column by column.\n",
    "    Denoting k the number of atoms in the dictionary and m the size of the signal, we have:\n",
    "    \n",
    "    Args:\n",
    "        D: dictionary of size (m,k)\n",
    "        A: Matrix of size (k,k)\n",
    "        B: Matrix of size (m,k)\n",
    "    Returns:\n",
    "        D: Updated dictionary of size (m,k)\n",
    "    '''\n",
    "    (m,k) = D.shape\n",
    "    assert A.shape == (k,k)\n",
    "    assert B.shape == (m,k)\n",
    "    \n",
    "    for j in range(k):        \n",
    "        uj = (B[:,j]-np.dot(D,A[:,j])) + D[:,j]\n",
    "        if A[j,j] != 0:\n",
    "            uj /= A[j,j]\n",
    "        else:\n",
    "            # TODO: What to do when A[j,j] is 0 ?\n",
    "            pass\n",
    "        D[:,j] = 1/max(np.linalg.norm(uj),1)*uj\n",
    "    return D\n",
    "\n",
    "def reconstruction_error(Y, D, X):\n",
    "    error = np.linalg.norm(Y - np.dot(D, X))**2\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img_size = 256\n",
    "filename = 'image.jpg'\n",
    "filename = 'barb_crop.png'\n",
    "filename = 'lena.bmp'\n",
    "f0 = load_image(filename, img_size)\n",
    "\n",
    "plt.figure(figsize = (6,6))\n",
    "imageplot(f0, 'Image f_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "width = 10\n",
    "signal_size = width*width\n",
    "n_atoms = 2*signal_size\n",
    "n_samples = 20*n_atoms\n",
    "k = 4 # Desired sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y, D0, X0 = datasets.make_sparse_coded_signal(n_samples, n_atoms, signal_size, k, random_state=0)\n",
    "D = np.random.random(D0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "omp = linear_model.OrthogonalMatchingPursuit(k, fit_intercept=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "D0 = high_energy_random_dictionary(f0, width, n_atoms)\n",
    "Y = random_dictionary(f0, width, n_samples)\n",
    "Y = center(Y) # TODO: Center because the dictionary is centered and no intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-SVD\n",
    "\n",
    "Aharon, Michal, Michael Elad, and Alfred Bruckstein. \"K-SVD: An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation.\" IEEE Transactions on signal processing 54.11 (2006): 4311-4322."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Backward\n",
    "\n",
    "Combettes, Patrick L., and Jean-Christophe Pesquet. \"Proximal splitting methods in signal processing.\" Fixed-point algorithms for inverse problems in science and engineering. Springer New York, 2011. 185-212.  \n",
    "\n",
    "Adapted from\n",
    "http://nbviewer.jupyter.org/github/gpeyre/numerical-tours/blob/master/matlab/sparsity_4_dictionary_learning.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_iter_learning = 100\n",
    "n_iter_dico = 50\n",
    "n_iter_coef = 100\n",
    "E = np.zeros(2*n_iter_learning)\n",
    "X = np.zeros((n_atoms, n_samples))\n",
    "D = D0\n",
    "for i in tqdm(range(n_iter_learning)):\n",
    "    # --- coefficient update ----\n",
    "    X = sparse_code_pgd(Y, D, X, sparsity=k, n_iter=n_iter_coef)\n",
    "    E[2*i] = reconstruction_error(Y, D, X)\n",
    "    # --- dictionary update ----\n",
    "    D = dictionary_update_pgd(Y, D, X, n_iter=n_iter_dico)\n",
    "    E[2*i+1] = reconstruction_error(Y, D, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove first points (burn in)\n",
    "start = 4\n",
    "assert start%2==0\n",
    "E_plot = E[start:]\n",
    "\n",
    "plt.plot(range(E_plot.shape[0]), E_plot)\n",
    "index_coef = list(range(0, E_plot.shape[0], 2))\n",
    "index_dico = list(range(1, E_plot.shape[0], 2))\n",
    "plt.plot(index_coef, E_plot[index_coef], '*', markersize=3, label='After coefficient update')\n",
    "plt.plot(index_dico, E_plot[index_dico], 'o', markersize=3, label='After dictionary update')\n",
    "plt.legend(numpoints=1)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('Error: $||Y-DX||^2$')\n",
    "plt.title('Projected gradient descent')\n",
    "plt.savefig('images/projected_gradient_descent.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,12))\n",
    "plot_dictionary(D0)\n",
    "plt.title('D0')\n",
    "plt.show()\n",
    "plt.figure(figsize=(8,12))\n",
    "plot_dictionary(D)\n",
    "plt.title('D')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online dictionary learning\n",
    "From \"Online Learning for Matrix Factorization and Sparse Coding\"  \n",
    "LARS-Lasso from LEAST ANGLE REGRESSION, Efron et al http://statweb.stanford.edu/~tibs/ftp/lars.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_iter = 5*n_samples\n",
    "test_interval = 1000\n",
    "lambd = 0.01 # L1 penalty coefficient for sparse coding\n",
    "lasso = linear_model.Lasso(lambd, fit_intercept=False) # TODO: use lars instead of lasso\n",
    "\n",
    "D = D0\n",
    "A = np.zeros((n_atoms,n_atoms))\n",
    "B = np.zeros((signal_size,n_atoms))\n",
    "\n",
    "sparsity = []\n",
    "E = []\n",
    "Y = random_dictionary(f0, width, 20*n_atoms)\n",
    "X = np.zeros((n_atoms, n_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in tqdm(range(n_iter)):\n",
    "    # Draw 1 random patch y and get its sparse coding\n",
    "    #y = random_dictionary(f0, width, n_atoms=1)\n",
    "    y = Y[:,np.random.randint(Y.shape[1])].reshape((signal_size,1))\n",
    "    x = lasso.fit(D, y).coef_.reshape((n_atoms,1))\n",
    "    A += np.dot(x,x.T)\n",
    "    B += np.dot(y,x.T)\n",
    "    D = dictionary_update_omf(D, A, B)\n",
    "    D = ProjC(D)\n",
    "    sparsity.append(np.mean(np.sum(x!=0, axis=0)))\n",
    "    \n",
    "    if i%test_interval == 0:\n",
    "        # Evaluation:\n",
    "        X = sparse_code_pgd(Y, D, X, sparsity=4, n_iter=100)\n",
    "        E.append(reconstruction_error(Y, D, X))\n",
    "        #sparsity.append(np.mean(np.sum(X!=0, axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(range(0, n_iter, test_interval), E)\n",
    "plt.title('Reconstruction error on the test set')\n",
    "plt.show()\n",
    "#plt.savefig('omf_2400000_iter.png')\n",
    "plt.figure(figsize=(8,12))\n",
    "plot_dictionary(D0)\n",
    "plt.title('D0')\n",
    "plt.show()\n",
    "plt.figure(figsize=(8,12))\n",
    "plot_dictionary(D)\n",
    "plt.title('D')\n",
    "plt.show()\n",
    "plt.plot(sparsity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = op.join('vars','omf_iter.out')\n",
    "with shelve.open(filename,'n') as shelf: # 'n' for new\n",
    "    for key in dir():\n",
    "        try:\n",
    "            shelf[key] = globals()[key]\n",
    "        except (TypeError, pickle.PicklingError, AttributeError):\n",
    "            #\n",
    "            # __builtins__, my_shelf, and imported modules can not be shelved.\n",
    "            #\n",
    "            print('ERROR shelving: {0}'.format(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x1 = np.array([[1,2,3]]).T\n",
    "x2 = np.array([[2,2,2]]).T\n",
    "X = np.array(np.hstack((x1,x2)))\n",
    "print(X.T)\n",
    "print(X)\n",
    "print(X.shape)\n",
    "print(np.dot(X, X.T))\n",
    "print(np.dot(x1,x1.T)+np.dot(x2,x2.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(np.dot(x1,x1.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
