{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code taken from  \n",
    "http://nbviewer.jupyter.org/github/gpeyre/numerical-tours/blob/master/python/inverse_5_inpainting_sparsity.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from nt_toolbox.signal import load_image, imageplot, snr\n",
    "from nt_toolbox.general import clamp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we consider inpainting of damaged observation without noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse Regularization\n",
    "---------------------\n",
    "We consider measurements $y=\\Phi f_0 + w$\n",
    "where $\\Phi$ is a masking operator\n",
    "and $w$ is an additive noise.\n",
    "\n",
    "\n",
    "This tour is focused on using sparsity to recover an image from the\n",
    "measurements $y$. It considers a synthesis-based regularization, that\n",
    "compute a sparse set of coefficients $ (a_m^{\\star})_m $\n",
    "in a frame $\\Psi = (\\psi_m)_m$ that solves\n",
    "$$a^{\\star} \\in \\text{argmin}_a \\: \\frac{1}{2}\\|y-\\Phi \\Psi a\\|^2 + \\lambda J(a)$$\n",
    "\n",
    "\n",
    "where $\\lambda$ should be adapted to the noise level $\\|w\\|$.\n",
    "Since in this tour we consider damaged observation without noise, i.e.\n",
    "$w=0$, we use either a very small value of $\\lambda$, or we decay its\n",
    "value through the iterations of the recovery process.\n",
    "\n",
    "\n",
    "Here we use the notation\n",
    "$$\\Psi a = \\sum_m a_m \\psi_m$$\n",
    "to indicate the reconstruction operator, and $J(a)$ is the $\\ell^1$\n",
    "sparsity prior\n",
    "$$J(a)=\\sum_m \\|a_m\\|.$$\n",
    "\n",
    "\n",
    "Missing Pixels and Inpainting\n",
    "-----------------------------\n",
    "Inpainting corresponds to filling holes in images.\n",
    "This corresponds to a linear ill posed inverse problem.\n",
    "\n",
    "\n",
    "You might want to do first the numerical tour _Variational image inpaiting_\n",
    "that use Sobolev and TV priors to performs the inpainting.\n",
    "\n",
    "\n",
    "First we load the image to be inpainted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img_size = 256\n",
    "f0 = load_image('image.jpg', img_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6,6))\n",
    "imageplot(f0, 'Image f_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amount of removed pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rho = .7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we construct a mask $\\Omega$ made of random pixel locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import random\n",
    "\n",
    "Omega = np.zeros([img_size, img_size])\n",
    "sel = random.permutation(img_size**2)\n",
    "np.ravel(Omega)[sel[np.arange(int(rho*img_size**2))]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The damaging operator put to zeros the pixel locations $x$ for which $\\Omega(x)=1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Phi = lambda f, Omega: f*(1-Omega)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The damaged observations reads $y = \\Phi f_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = Phi(f0, Omega)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6,6))\n",
    "imageplot(y, 'Observations y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary initialization inspired from  \n",
    "http://nbviewer.jupyter.org/github/gpeyre/numerical-tours/blob/master/matlab/sparsity_4_dictionary_learning.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = 10   # Width of the patches\n",
    "m = w*w  # Size of the signal to be sparse coded\n",
    "k = 2*m  # Number of atoms in the dictionary (overcomplete)\n",
    "n = 20*k # Number of training samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a random patch in the damaged image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def random_patch(image, width, n_patches=1):\n",
    "    img_shape = image.shape\n",
    "    # Upper left corners of patches\n",
    "    rows = np.random.randint(0, img_shape[0]-width, n_patches)\n",
    "    cols = np.random.randint(0, img_shape[1]-width, n_patches)\n",
    "    \n",
    "    patches = np.zeros((n_patches, width, width))\n",
    "    for i in range(n_patches):\n",
    "        patches[i] = image[\n",
    "            rows[i]:rows[i]+width,\n",
    "            cols[i]:cols[i]+width\n",
    "        ]\n",
    "    return patches\n",
    "\n",
    "def plot_dictionary(D):\n",
    "    assert len(D.shape) == 3\n",
    "    assert D.shape[1] == D.shape[2]\n",
    "    n_patches = D.shape[0]\n",
    "    patch_size = D.shape[1]\n",
    "    n = int(np.ceil(np.sqrt(n_patches))) # Size of the square in number of patches\n",
    "\n",
    "    # Pad the images\n",
    "    pad_size = 1\n",
    "    missing_patches = n ** 2 - n_patches\n",
    "\n",
    "    padding = (((0, missing_patches),\n",
    "                (pad_size, pad_size), (pad_size, pad_size)))\n",
    "    D = np.pad(D, padding, mode='constant', constant_values=1)\n",
    "    padded_patch_size = patch_size + 2*pad_size\n",
    "    D = D.reshape(n,n,padded_patch_size,padded_patch_size)\n",
    "    D = D.transpose(0,2,1,3) # Needed for the reshape\n",
    "    big_image_size = n*padded_patch_size\n",
    "    D = D.reshape(big_image_size, big_image_size)\n",
    "    imageplot(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm 2 Dictionary update\n",
    "From \"Online Learning for Matrix Factorization and Sparse Coding\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_dictionary(D):\n",
    "    # TODO: To complete\n",
    "    return D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm 1 Online dictionary learning\n",
    "From \"Online Learning for Matrix Factorization and Sparse Coding\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn import linear_model\n",
    "\n",
    "# Initialize variables\n",
    "T = 100 # Number of iterations\n",
    "lambd = 0.1 # L1 penalty coefficient for alpha\n",
    "# LARS-Lasso from LEAST ANGLE REGRESSION, Efron et al http://statweb.stanford.edu/~tibs/ftp/lars.pdf\n",
    "lasso = linear_model.Lasso(lambd, fit_intercept=False) # TODO: use lars instead of lasso\n",
    "\n",
    "D = random_patch(y, w, n_patches=k) # Initialize dictionary with k random atoms\n",
    "D = D.reshape(k, m).T # Reshape each atom to column vector\n",
    "# TODO: normalize atom to unit norm as sparsity_4_dictionary_learning ?\n",
    "A = np.zeros((k,k))\n",
    "B = np.zeros((m,k))\n",
    "sparsity = np.zeros(T)\n",
    "\n",
    "start = time.time()\n",
    "for t in range(T):\n",
    "    x = random_patch(y, w, n_patches=1).reshape((m,1)) # Draw 1 random patch as column vector\n",
    "    alpha = lasso.fit(D, x).coef_.reshape((k,1)) # Get the sparse coding # TODO: try with lasso.sparse_coef_\n",
    "    sparsity[t] = np.sum(alpha!=0)#/alpha.shape[0]\n",
    "    A += alpha*alpha.T\n",
    "    B += x*alpha.T\n",
    "    D = update_dictionary(D)\n",
    "end = time.time()\n",
    "\n",
    "print('Time elapsed: %.3f s' % (end-start))\n",
    "plot_dictionary(D.T.reshape(k, w, w))\n",
    "sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soft Thresholding in a Basis\n",
    "----------------------------\n",
    "The soft thresholding operator is at the heart of $\\ell^1$ minimization\n",
    "schemes. It can be applied to coefficients $a$, or to an image $f$\n",
    "in an ortho-basis.\n",
    "\n",
    "\n",
    "The soft thresholding is a 1-D functional that shrinks the value of\n",
    "coefficients.\n",
    "$$ s_T(u)=\\max(0,1-T/|u|)u $$\n",
    "\n",
    "\n",
    "Define a shortcut for this soft thresholding 1-D functional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SoftThresh = lambda x, T: x*np.maximum(1-T/np.maximum(abs(x), 1e-10*np.ones(np.shape(x))), np.zeros(np.shape(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display a curve of the 1D soft thresholding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 1, 1000)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(x, SoftThresh(x,.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the function SoftThresh can also be applied to vector which defines an\n",
    "operator on coefficients:\n",
    "$$ S_T(a) = ( s_T(a_m) )_m. $$\n",
    "\n",
    "\n",
    "In the next section, we use an orthogonal wavelet basis $\\Psi$.\n",
    "\n",
    "\n",
    "We set the parameters of the wavelet transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Jmax = np.log2(n)-1\n",
    "Jmin = (Jmax-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shortcut for $\\Psi$ and $\\Psi^*$ in the orthogonal case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nt_toolbox.perform_wavelet_transf import *\n",
    "\n",
    "Psi = lambda a: perform_wavelet_transf(a, Jmin, -1, ti=0)\n",
    "PsiS = lambda f: perform_wavelet_transf(f, Jmin, +1, ti=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The soft thresholding opterator in the basis $\\Psi$ is defined as\n",
    "$$S_T^\\Psi(f) = \\sum_m s_T( \\langle f,\\psi_m \\rangle ) \\psi_m $$\n",
    "\n",
    "\n",
    "It thus corresponds to applying the transform $\\Psi^*$, thresholding\n",
    "the coefficients using $S_T$ and then undoing the transform using\n",
    "$\\Psi$.\n",
    "$$ S_T^\\Psi(f) = \\Psi \\circ S_T \\circ \\Psi^*$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SoftThreshPsi = lambda f, T: Psi(SoftThresh(PsiS(f), T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This soft thresholding corresponds to a denoising operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "imageplot(clamp(SoftThreshPsi(f0, 0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inpainting using Orthogonal Wavelet Sparsity\n",
    "--------------------------------------------\n",
    "If $\\Psi$ is an orthogonal basis, a change of variable shows that the\n",
    "synthesis prior is also an analysis prior, that reads\n",
    "$$f^{\\star} \\in \\text{argmin}_f \\: E(f) = \\frac{1}{2}\\|y-\\Phi f\\|^2 + \\lambda \\sum_m \\|\\langle f,\\psi_m \\rangle\\|. $$\n",
    "\n",
    "\n",
    "To solve this non-smooth optimization problem, one can use\n",
    "forward-backward splitting, also known as iterative soft thresholding.\n",
    "\n",
    "\n",
    "It computes a series of images $f^{(\\ell)}$ defined as\n",
    "$$ f^{(\\ell+1)} = S_{\\tau\\lambda}^{\\Psi}( f^{(\\ell)} - \\tau \\Phi^{*} (\\Phi f^{(\\ell)} - y)  ) $$\n",
    "\n",
    "\n",
    "Set up the value of the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lambd = .03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our setting, we have $ \\Phi^* = \\Phi $ which is an operator of norm\n",
    "1.\n",
    "\n",
    "\n",
    "For $f^{(\\ell)}$ to converge to a solution of the problem, the gradient\n",
    "step size should be chosen as\n",
    "$$\\tau < \\frac{2}{\\|\\Phi^* \\Phi\\|} = 2$$\n",
    "\n",
    "\n",
    "In the following we use:\n",
    "$$\\tau = 1$$\n",
    "\n",
    "\n",
    "Since we use $ \\tau=1 $ and $ \\Phi = \\Phi^* = \\text{diag}(1-\\Omega) $,  the gradient descent step\n",
    "is a projection on the inpainting constraint\n",
    "$$ C = \\{ f \\backslash \\forall \\Omega(x)=0, f(x)=y(x) \\} $$\n",
    "One thus has\n",
    "$$ f - \\tau \\Phi^{*} (\\Phi f - y)  = \\text{Proj}_C(f) $$\n",
    "\n",
    "\n",
    "For the sake of simplicity, we define a shortcut for this projection\n",
    "operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ProjC = lambda f, Omega: Omega*f + (1-Omega)*y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each iteration of the forward-backward (iterative thresholding) algorithm\n",
    "thus reads:\n",
    "$$ f^{(\\ell+1)} = S_{\\lambda}^\\Psi( \\text{Proj}_C(f^{(\\ell)}) ). $$\n",
    "\n",
    "\n",
    "Initialize the iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fSpars = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step: gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fSpars = ProjC(fSpars, Omega)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second step: denoise the solution by thresholding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fSpars = SoftThreshPsi(fSpars, lambd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 1__\n",
    "\n",
    "Perform the iterative soft thresholding.\n",
    "Monitor the decay of the energy $E$ you are minimizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run -i nt_solutions/inverse_5_inpainting_sparsity/exo1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Insert your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "imageplot(clamp(fSpars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 2__\n",
    "\n",
    "Since there is no noise, one should in theory take $\\lambda\n",
    "\\rightarrow 0$.\n",
    "To do this, decay the value of $\\lambda$ through the iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run -i nt_solutions/inverse_5_inpainting_sparsity/exo2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Insert your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inpainting using Translation Invariant Wavelet Sparsity\n",
    "-------------------------------------------------------\n",
    "Orthogonal sparsity performs a poor regularization because of the lack of\n",
    "translation invariance. This regularization is enhanced by considering\n",
    "$\\Psi$ as a redundant tight frame of translation invariant wavelets.\n",
    "\n",
    "\n",
    "One thus looks for optimal coefficients $a^\\star$ that solves\n",
    "$$a^{\\star} \\in \\text{argmin}_a \\: E(a) = \\frac{1}{2}\\|y-\\Phi \\Psi a\\|^2 + \\lambda J(a)$$\n",
    "\n",
    "\n",
    "*Important*: The operator $\\Psi^*$ is the forward translation invariant wavelet transform.\n",
    "It computes the inner product with the unit norm wavelet atoms:\n",
    "$$ (\\Psi^* f)_m = \\langle f,\\psi_m \\rangle \\quad \\text{with} \\quad \\|\\psi_m\\|=1. $$\n",
    "\n",
    "\n",
    "The reconstruction operator $\\Xi$ satisfies $ \\Xi \\Psi^* f = f $, and\n",
    "is the pseudo inverse of the analysis operator $ \\Xi = (\\Psi^*)^+ $.\n",
    "\n",
    "\n",
    "For our algorithm, we will need to use $\\Psi$ and not $\\Xi$. Lukily,\n",
    "for the wavelet transform, one has\n",
    "$$ \\Xi = \\Psi \\text{diag(U)} f $$\n",
    "where $U_m$ account for the redundancy of the scale of the atom\n",
    "$\\psi_m$.\n",
    "\n",
    "\n",
    "Compute the scaling factor (inverse of the redundancy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "J = Jmax-Jmin + 1\n",
    "u = np.hstack(([4**(-J)], 4**(-np.floor(np.arange(J + 2./3,1,-1./3)))))\n",
    "U = np.transpose(np.tile(u, (n,n,1)),(2,0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a value of the regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lambd = .01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shortcut for the wavelet transform and the reconstruction.\n",
    "\n",
    "\n",
    "\n",
    "*Important:* Scilab users have to create files |Xi.m|, |PsiS.m| and |Psi.m| to implement this\n",
    "function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xi = lambda a: perform_wavelet_transf(a, Jmin, -1, ti=1)\n",
    "PsiS = lambda f: perform_wavelet_transf(f, Jmin, + 1, ti=1)\n",
    "Psi = lambda a: Xi(a/U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward-backward algorithm now compute a series of wavelet\n",
    "coefficients $a^{(\\ell)}$ computed as\n",
    "$$a^{(\\ell+1)} = S_{\\tau\\lambda}( a^{(\\ell)} + \\Psi^*\\Phi( y - \\Phi\\Psi a^{(\\ell)} )  ). $$\n",
    "\n",
    "\n",
    "The soft thresholding is defined as:\n",
    "$$\\forall m, \\quad S_T(a)_m = \\max(0, 1-T/\\|a_m\\|)a_m. $$\n",
    "\n",
    "\n",
    "The step size should satisfy:\n",
    "$$\\tau < \\frac{2}{\\|\\Psi\\Phi \\|} \\leq 2 \\min( u ). $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tau = 1.9*np.min(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the wavelet coefficients with those of the previous reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = U*PsiS(fSpars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fTI = Psi(a)\n",
    "a = a + tau*PsiS(Phi(y-Phi(fTI, Omega), Omega))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soft threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = SoftThresh(a, lambd*tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 3__\n",
    "\n",
    "Perform the iterative soft thresholding. Monitor the decay of the\n",
    "energy $E$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run -i nt_solutions/inverse_5_inpainting_sparsity/exo3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Insert your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fTI = Psi(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "imageplot(clamp(fTI))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 4__\n",
    "\n",
    "Perform the iteration with a decaying value of $\\lambda$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run -i nt_solutions/inverse_5_inpainting_sparsity/exo4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Insert your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inpainting using Iterative Hard Thresholding\n",
    "--------------------------------------------\n",
    "To improve the sparsity of the solution, it is possible to replace the\n",
    "soft thresholding by a hard threshdoling. In this case, the resulting\n",
    "algorihtm does not perform anymore a variational minimization of an\n",
    "energy.\n",
    "\n",
    "\n",
    "The hard thresholding is defined as $h_T(x)=0$ if $-T < x < T$\n",
    "and $h_T(x)=x$ otherwise. It thus defines a thresholding operator of\n",
    "wavelet coefficients as $H_T(a)_m = h_T(a_m)$.\n",
    "\n",
    "\n",
    "Define a shortcut for this vectorialized hard thresholding\n",
    "\n",
    "\n",
    "\n",
    "*Important:* Scilab users have to create a file |HardThresh.m| to implement this\n",
    "function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HardThresh = lambda x, t: x*(abs(x) > t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display a curve of the 1-D Hard thresholding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 1, 1000)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(x, HardThresh(x, .5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hard thresholding in the translation invariant wavelet basis $\\Psi$\n",
    "reads\n",
    "$$ H_T^\\Psi(f) = \\Xi \\circ H_T \\circ \\Psi^* (f) $$\n",
    "where $\\Xi = (\\Phi^*)^+$ is the reconstruction operator.\n",
    "\n",
    "\n",
    "We follow the MCA paradigm of Jean-Luc Starck, that alternates between a\n",
    "gradient descent step and a hard thresholding denoising, using a decaying\n",
    "threshold.\n",
    "$$f^{(\\ell+1)} = H_{\\tau\\lambda_\\ell}^\\Psi( f^{(\\ell)} - \\tau \\Phi^*(\\Phi f^{(\\ell)} - y)  ). $$\n",
    "\n",
    "\n",
    "Number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "niter = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of thresholds. One must start by a large enough initial threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lambda_list = np.linspace(1, 0, niter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fHard = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fHard = ProjC(fHard, Omega)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard threshold (here $\\lambda=\\lambda_0$) is used)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fHard = Xi(HardThresh(PsiS(fHard), tau*lambda_list[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 5__\n",
    "\n",
    "Perform the iteration with a decaying value of $\\lambda$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run -i nt_solutions/inverse_5_inpainting_sparsity/exo5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Insert your code here."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
